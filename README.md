# Desafio de Engenharia de Dados
üìå Status do Projeto: Resolvido

Este reposit√≥rio cont√©m a solu√ß√£o para o desafio de Engenharia de Dados proposto por um Laborat√≥rio de Tecnologia bastante conhecido no mercado.

## Descri√ß√£o do Projeto

O projeto est√° dividido em duas partes principais:
1.  **Desafio 1:** Modelagem de dados de um JSON de ERP para um esquema relacional SQL, com a devida justificativa.
2.  **Desafio 2:** Proposta de arquitetura para ingest√£o e armazenamento de dados em um Data Lake, abordando estrutura, governan√ßa e resili√™ncia a mudan√ßas de esquema.

## Como Executar o Projeto

Siga os passos abaixo para configurar e executar os scripts da solu√ß√£o.

### Pr√©-requisitos
- Python 3.8+
- Git

### Passos

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone [https://github.com/edu-data-dev/Desafio-dados-erp.git](https://github.com/edu-data-dev/Desafio-dados-erp.git)
    cd desafio-engenharia-de-dados
    ```

2.  **Crie e ative um ambiente virtual (recomendado):**
    ```bash
    # Cria o ambiente
    python -m venv .venv

    # Ativa no Windows
    # .venv\Scripts\activate

    # Ativa no macOS/Linux
    source .venv/bin/activate
    ```

3.  **Executar a solu√ß√£o do Desafio 1:**
    Este script ir√° criar um banco de dados SQLite (`coco_bambu.db`) e popul√°-lo com os dados do `data/ERP.json`.
    ```bash
    python src/challenge_1.py
    ```

4.  **Executar a simula√ß√£o do Desafio 2:**
    Este script ir√° criar uma pasta `datalake` na raiz do projeto, simulando a ingest√£o de arquivos na estrutura particionada proposta.
    ```bash
    python src/challenge_2.py
    ```
## Estrutura do Reposit√≥rio

- **/data**: Cont√©m os dados brutos de entrada (ex: `ERP.json`).
- **/sql**: Cont√©m os scripts de DDL (Data Definition Language) para criar o schema no banco de dados.
- **/src**: Cont√©m os scripts Python para processamento e simula√ß√£o.
- **README.md**: Este documento.

## Solu√ß√£o do Desafio

*(Aqui voc√™ vai colocar ou linkar suas respostas detalhadas)*

### Desafio 1

#### 1. Esquema do JSON

O arquivo `ERP.json` representa a resposta de um endpoint de ERP para um pedido espec√≠fico. Sua estrutura hier√°rquica √© a seguinte:

- **Objeto Raiz**:
  - `curUTC` (string): Timestamp atual em UTC.
  - `locRef` (string): Identificador da loja/local.
  - `guestChecks` (array): Lista contendo os pedidos. No exemplo, h√° apenas um.
    - **Objeto `guestCheck`**:
      - `guestCheckId` (number): ID √∫nico do pedido.
      - `chkNum` (number): N√∫mero do pedido.
      - `opnBusDt`, `clsdBusDt` (string): Datas de neg√≥cio de abertura e fechamento.
      - `opnUTC`, `clsdUTC` (string): Timestamps de abertura e fechamento em UTC.
      - `clsdFlag` (boolean): Indica se a conta est√° fechada.
      - `gstCnt` (number): N√∫mero de clientes.
      - `subTtl`, `chkTtl`, `dscTtl`, `payTtl` (number): Valores monet√°rios totais.
      - `tblNum`, `empNum` (number): IDs da mesa e do funcion√°rio.
      - `taxes` (array): Lista dos impostos aplicados ao pedido.
        - **Objeto `tax`**:
          - `taxNum` (number): ID do imposto.
          - `txblSlsTtl` (number): Valor sobre o qual o imposto incide.
          - `taxCollTtl` (number): Valor do imposto coletado.
          - `taxRate` (number): Al√≠quota do imposto.
      - `detailLines` (array): Lista de itens/linhas que comp√µem o pedido.
        - **Objeto `detailLine`**:
          - `guestCheckLineItemId` (number): ID √∫nico da linha do item.
          - `dspTtl`, `aggTtl` (number): Valor do item.
          - `dspQty`, `aggQty` (number): Quantidade do item.
          - `menuItem` (objeto): Detalhes do item do card√°pio.
            - **Objeto `menuItem`**:
              - `miNum` (number): ID do item no card√°pio.
              - `modFlag` (boolean): Indica se √© uma modifica√ß√£o de um item.
              - `inclTax` (number): Valor do imposto incluso no pre√ßo.
...
#### 2. Transcri√ß√£o para Tabelas SQL

O script para cria√ß√£o das tabelas se encontra em `sql/schema.sql`.


#### 3. Justificativa da Abordagem de Modelagem

A abordagem escolhida para transcrever o JSON para SQL foi a **normaliza√ß√£o em um modelo relacional (3¬™ Forma Normal - 3NF)**. A principal motiva√ß√£o √© criar um esquema que seja escal√°vel, √≠ntegro e otimizado para as opera√ß√µes de um restaurante.

- **Integridade e Redu√ß√£o de Redund√¢ncia**:
  - Cada entidade l√≥gica (pedidos, impostos, itens de linha) foi separada em sua pr√≥pria tabela. A tabela `guest_checks` armazena informa√ß√µes do pedido uma √∫nica vez, evitando a repeti√ß√£o desses dados para cada item consumido.
  - O uso de chaves prim√°rias (`PRIMARY KEY`) e estrangeiras (`REFERENCES`) garante a **integridade referencial**. Por exemplo, um registro em `detail_lines` n√£o pode existir sem um `guest_check_id` correspondente na tabela `guest_checks`, prevenindo dados √≥rf√£os.

- **Escalabilidade**:
  - A estrutura foi pensada para abranger toda a cadeia de restaurantes. A inclus√£o do campo `location_ref` na tabela `guest_checks` permite que os dados de todas as lojas sejam armazenados no mesmo banco de dados, podendo ser facilmente filtrados ou agregados por unidade.
  - Consultas de BI, como "Qual o item mais vendido em todas as lojas no √∫ltimo m√™s?", tornam-se eficientes atrav√©s de `JOINs` e agrega√ß√µes, o que seria computacionalmente caro de se fazer processando milhares de arquivos JSON.

- **Flexibilidade e Extensibilidade**:
  - O desafio menciona que o objeto `detailLines` pode conter diferentes tipos de informa√ß√£o (`menuItem`, `discount`, `serviceCharge`). Para lidar com isso, a tabela `detail_lines` foi projetada de forma **polim√≥rfica**.
  - A coluna `line_type` identifica a natureza de cada registro (ex: 'MENU_ITEM'). Colunas de chave estrangeira como `menu_item_number` (e futuras como `discount_id`) seriam preenchidas condicionalmente. Isso permite que novas categorias de linhas de detalhe sejam adicionadas no futuro sem a necessidade de reestruturar as tabelas existentes, apenas adicionando novas colunas ou tabelas de refer√™ncia.
...


### Desafio 2

Esta se√ß√£o aborda a arquitetura de um Data Lake para armazenar e processar dados de m√∫ltiplos endpoints da API de restaurantes.

#### 1. Por que armazenar as respostas das APIs?

Armazenar as respostas brutas das APIs (JSONs) em uma camada inicial do Data Lake, conhecida como **Raw Zone** ou **Landing Zone**, √© uma pr√°tica fundamental na engenharia de dados moderna. As principais raz√µes para isso s√£o:

- **Fonte da Verdade (Source of Truth)**: Os dados brutos s√£o uma c√≥pia fiel e imut√°vel dos dados na origem. Eles servem como uma garantia permanente, permitindo auditorias e a reconstru√ß√£o de qualquer camada de dados processada a partir deles.
- **Reprocessamento (Replayability)**: Regras de neg√≥cio, corre√ß√µes de bugs ou novas necessidades de an√°lise inevitavelmente surgir√£o. Com os dados brutos guardados, podemos "reprocessar" o hist√≥rico com a nova l√≥gica a qualquer momento, sem a necessidade de consultar as APIs de origem novamente, o que poderia ser custoso ou at√© imposs√≠vel.
- **Flexibilidade e Futuros Casos de Uso**: Os dados brutos cont√™m a totalidade das informa√ß√µes. Isso permite que equipes de Business Intelligence, Analytics e Ci√™ncia de Dados explorem os dados para novos insights que n√£o foram previstos no momento da ingest√£o. O modelo √© de **Schema-on-Read** (o esquema √© aplicado na leitura), e n√£o na escrita.
- **Desacoplamento de Sistemas**: A pipeline √© dividida em est√°gios. O processo de ingest√£o (salvar o JSON) √© simples e robusto, separado do processo de transforma√ß√£o, que √© mais complexo e sujeito a falhas. Se a transforma√ß√£o falhar, a coleta de dados n√£o √© interrompida, evitando perda de dados.

#### 2. Como voc√™ armazenaria os dados? (Estrutura de Pastas) 

Para armazenar as respostas da API de forma a permitir manipula√ß√£o, buscas e pesquisas r√°pidas, a melhor abordagem √© uma **estrutura de pastas hier√°rquica e particionada**. O particionamento √© a chave para a performance em um Data Lake.

A estrutura proposta seria: 
    /datalake/raw/{source_name}/{api_endpoint}/year={YYYY}/month={MM}/day={DD}/{store_id}/data.json

**Exemplo Pr√°tico:**

/datalake/raw/erp_api/getGuestChecks/year=2025/month=08/day=27/store_id=99_CB_CB/1661608800_response.json
/datalake/raw/erp_api/getFiscalInvoice/year=2025/month=08/day=27/store_id=101_BSB/1661608800_response.json

**Justificativa da Estrutura:**

- `raw`: Define a camada de dados (brutos, n√£o processados).
- `{source_name}` (ex: `erp_api`): Agrupa os dados por sistema de origem.
- `{api_endpoint}` (ex: `getGuestChecks`): Isola os dados de cada endpoint, pois eles possuem esquemas diferentes[cite: 35].
- `year={YYYY}/month={MM}/day={DD}`: **Particionamento por data**. Esta √© a otimiza√ß√£o mais cr√≠tica. Ferramentas de consulta como Apache Spark, Presto ou Athena usam o "partition pruning" para ler apenas as pastas relevantes para uma consulta (ex: "vendas do √∫ltimo m√™s"), ignorando terabytes de outros dados e tornando as consultas ordens de magnitude mais r√°pidas. O formato `chave=valor` √© um padr√£o de mercado.
- `{store_id}`: Particionar tamb√©m pela loja [cite: 36] √© extremamente √∫til, pois muitas an√°lises de neg√≥cio s√£o segmentadas por unidade.
- `data.json`: O arquivo em si, talvez com um timestamp ou UUID para garantir a unicidade.

#### 3. Implica√ß√µes da mudan√ßa de `guestChecks.taxes` para `guestChecks.taxation` 

Este cen√°rio descreve um **Schema Drift** (desvio de esquema), um desafio comum em engenharia de dados. As implica√ß√µes ocorreriam em cascata:

- **Na Camada de Ingest√£o (Raw Zone)**: **Nenhuma implica√ß√£o.** O processo de ingest√£o simplesmente salvaria o novo arquivo JSON com o campo `taxation`. A beleza da Raw Zone √© ser "schema-agn√≥stica". Nenhuma informa√ß√£o seria perdida.

- **Na Camada de Transforma√ß√£o (Processing Zone)**: **Falha Cr√≠tica.** √â aqui que o problema se manifesta. O c√≥digo (Spark, Python, etc.) que l√™ os JSONs para transform√°-los em tabelas estruturadas (como as que criamos no Desafio 1) est√° programado para encontrar o campo `taxes`.
    - **Resultado 1 (Falha Expl√≠cita):** O job de transforma√ß√£o falharia com um erro do tipo `KeyError: 'taxes'`, paralisando a atualiza√ß√£o dos dados a partir do momento da mudan√ßa.
    - **Resultado 2 (Falha Silenciosa):** Se o c√≥digo for escrito de forma "defensiva" (ex: `guest_check.get('taxes', [])`), ele n√£o quebraria, mas passaria a inserir dados de impostos nulos ou vazios, corrompendo silenciosamente as m√©tricas de neg√≥cio.

- **Na Camada de Consumo (BI e Usu√°rios Finais)**: Os dashboards, relat√≥rios e an√°lises que dependem dos dados de impostos come√ßariam a exibir valores incorretos (zeros ou nulos), levando a decis√µes de neg√≥cio erradas e √† perda de confian√ßa nos dados.

**Como Mitigar:**
1.  **C√≥digo Resiliente:** A l√≥gica de transforma√ß√£o deve ser adaptada para lidar com ambas as vers√µes do esquema. Ex: `tax_data = guest_check.get('taxation', guest_check.get('taxes', []))`.
2.  **Monitoramento e Alertas:** Implementar um sistema que valide o esquema dos dados que chegam na Raw Zone e dispare alertas para a equipe de dados sempre que um desvio for detectado.
3.  **Schema Registry:** Em sistemas mais maduros, ferramentas como o Confluent Schema Registry podem ser usadas para gerenciar e versionar esquemas de dados, permitindo que os processos de transforma√ß√£o se adaptem dinamicamente.


